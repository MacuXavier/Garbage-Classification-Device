{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "544a77e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import matplotlib.pyplot as plt     \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15a939f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset\\\\Hazardous_waste\\\\haz_bet1.jpeg',\n",
       " 'Dataset\\\\Hazardous_waste\\\\haz_bet10.jpeg',\n",
       " 'Dataset\\\\Hazardous_waste\\\\haz_bet100.jpeg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = glob.glob('Dataset/*/*')     #寻找数据集的路径\n",
    "image_path[:3]                      #前三张图片的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef3deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hazardous_waste' 'Kitchen_waste' 'Other_trash' 'Recyclable_trash']\n"
     ]
    }
   ],
   "source": [
    "all_label_name = [img_p.split('\\\\')[1] for img_p in image_path]     #获取类别名，要用[1]\n",
    "label_name = np.unique(all_label_name)         #np.unique()函数是将重复的，去除重复类别名\n",
    "print(label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d55fdc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hazardous_waste': 0,\n",
       " 'Kitchen_waste': 1,\n",
       " 'Other_trash': 2,\n",
       " 'Recyclable_trash': 3}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#英文标签转为数字标签\n",
    "label_to_index = dict((name, i) for i,name in enumerate(label_name))   #enumerrate生成(索引,名称)\n",
    "label_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4707a3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Hazardous_waste',\n",
       " 1: 'Kitchen_waste',\n",
       " 2: 'Other_trash',\n",
       " 3: 'Recyclable_trash'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_label = dict((f, q) for q,f in label_to_index.items())  #把上面的数字标签位置换一下，items\n",
    "index_to_label                                               #并转化为dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f756bd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_index = [label_to_index.get(name) for name in all_label_name]   #获取字典的键对值\n",
    "all_index[-5:]      #展示最后五张键对值来check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b7a03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机打乱图片路径和标签\n",
    "np.random.seed(2021)  #设置随机数\n",
    "random_index = np.random.permutation(len(image_path))  #随机排序，index\n",
    "image_path = np.array(image_path)[random_index]  #np.array主要是将数据转化成数组\n",
    "all_index = np.array(all_index)[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5f51a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = int(len(image_path)*0.8)    #拿80%图片来训练，20%图片来做测试\n",
    "image_train = image_path[:num]     #train dataset\n",
    "index_train = all_index[:num]     #训练标签图片路径\n",
    "image_val = image_path[num:]     #test dataset\n",
    "index_val = all_index[num:]      #测试标签图片路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "130e675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1899, 1) (475, 1)\n"
     ]
    }
   ],
   "source": [
    "#expand_dims的第二个参数，主要是选择在第几维度增加维度，-1即在最后一维增加维度\n",
    "#这里是index_train,而不是train_path\n",
    "index_train = tf.expand_dims(index_train, -1)     #给训练图片索引增加一个维度\n",
    "index_val = tf.expand_dims(index_val, -1)        #给测试集索引都增加一个维度\n",
    "print(index_train.shape, index_val.shape)     \n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((image_train, index_train))#加载训练dataset，一个是train_path,另外一个是train_index\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((image_val, index_val))#加载测试dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "294a0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path,label):   #函数用来获取图片并作数据增强\n",
    "    image = tf.io.read_file(path)   #这个是读取并输出文件名的全部内容\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  #对图片进行解码，即把图片转化成tensor,图片是RGB三通道的所以channels=3\n",
    "    image = tf.image.resize(image, [64, 64])   #将图片裁剪成64X64的图片，默认的裁剪方式一般是双线性插值，也可以选择其他方式但要在工程里修改\n",
    "    # 数据增强(随机翻转)\n",
    "    image = tf.image.random_flip_left_right(image)  #将图片进行随机左右旋转，拓展训练集数量，用来减轻过拟合现象\n",
    "    image = tf.image.random_flip_up_down(image)    #对图片进行随机上下旋转\n",
    "    # image = tf.image.random_crop(image, [64, 64, 3])\n",
    "    image = tf.cast(image, tf.float32)/255   #数据归一化，1.提高训练速度，2.提高模型精度，具体请百度，之所以除以255，是因为图片像素是从0到255\n",
    "    label = tf.convert_to_tensor(label)    #将标签转化成tensor\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2783b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16   #这里batch_size是16，每次读取图片数量是16张，和显存有关,\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE #多线程设置，自动模式，使用多线程进行训练\n",
    "dataset_train = dataset_train.map(load_image, num_parallel_calls=AUTOTUNE)\n",
    "dataset_train = dataset_train.batch(BATCH_SIZE)\n",
    "dataset_val = dataset_val.map(load_image, num_parallel_calls=AUTOTUNE)\n",
    "dataset_val = dataset_val.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7935132c",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "786599dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CNN\n",
    "#使用卷积神经网络，构建模型都是使用tensorflow的layers类来进行构建的\n",
    "def CNNmodel(input_shape,filters=64, kernel=(3,3),size=4,dropout=0.2,**kwargs):\n",
    "    #input_shape是输入图片尺寸，因为之前做了resize，所以这里都是64乘64乘3的图片，\n",
    "    #filters是卷积核数量，kernel是卷积核大小，dropout是随机丢弃方法，0.2是随机丢弃的概率，\n",
    "    #为了可以在for循环中多执行几次\n",
    "    _inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(8,(3,3),padding='same',use_bias=False,strides=(2,2), name='conv_0')(_inputs)\n",
    "    #二维卷积，不太明白的是第一个参数为什么是8？这个参数在API中对应filters,也就是输出通道数，图片原先通道数是3现在变成了8\n",
    "    #padding='same',所以这里输出图片尺寸32*32，\n",
    "    x = layers.BatchNormalization(axis=-1, name='conv_0_bn')(x)#加了一个batchnorm\n",
    "    x = layers.ReLU(6., name='conv_0_relu')(x)#加上了Relu激活函数，增加非线性\n",
    "    \n",
    "    x = layers.AveragePooling2D(name='avg_1')(x)#平均池化层，浅显来说就是将图片长、宽减半的层，具体的作用方式和卷积造成的图片长宽减半不同，目的是降维\n",
    "    #平均池化后，图片尺寸变成了16乘以16，通道数和上面一致，为8\n",
    "    x = layers.Dropout(dropout, name='dropout_1')(x)#随机丢弃层\n",
    "\n",
    "    for block_id in range(2,size+2):#多卷积几次，卷积的目的实际上是提取特征，一般来说，卷积之后是要做batchnorm，添加激活函数，然后做dropout\n",
    "        x = layers.Conv2D(filters,kernel,padding='same',use_bias=False,strides=(1,1), name='conv_%d'%block_id)(x)\n",
    "        #这里的卷积应该是用来做特征提取的，for循环最后输出尺寸是16乘以16，通道数是8\n",
    "        x = layers.BatchNormalization(axis=-1, name='conv_%d_bn'%block_id)(x)#batchnorm\n",
    "        x = layers.ReLU(6., name='conv_%d_relu'%block_id)(x)#添加激活函数\n",
    "\n",
    "    x = layers.Conv2D(64,(3, 3),padding='same',use_bias=False,strides=(2,2), name='conv_1')(x)\n",
    "    #卷积核数量64，意味输出通道数是64，padding='same'，所以现在输出图片尺寸是8乘8，通道数是64\n",
    "    x = layers.AveragePooling2D()(x)#又做了一层平均池化，所以图片尺寸是4乘4，通道数是64\n",
    "    x = layers.Dropout(dropout, name='dropout_2')(x)\n",
    "    x = layers.GlobalAveragePooling2D(name='avg_2')(x)#全局平均池化，卷积提取特征最后一步\n",
    "    x = layers.Dropout(dropout, name='dropout_0')(x)\n",
    "    x = layers.Dense(4)(x)#全连接层，分类作用\n",
    "    x = layers.Softmax()(x)#softmax函数，将softmax用于多分类过程中，将多个输入映射到（0,1）区间内\n",
    "    return keras.Model(inputs=_inputs,outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a3a9af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='accuracy', factor=0.5, patience=4, min_lr=0.0001,verbose=1)\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=8,verbose=1)#这两行主要作用是检测训练的过程\n",
    "\n",
    "model = CNNmodel(input_shape=(64,64,3),filters=64, kernel=(3,3),size=3)#定义模型参数\n",
    "model.compile(optimizer='SGD',loss='sparse_categorical_crossentropy',metrics=['accuracy'])#使用SGD优化器，损失函数，评价指标accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ea4ed71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 - 4s - loss: 1.0877 - accuracy: 0.5271 - val_loss: 1.4423 - val_accuracy: 0.2358 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset_train,validation_data=dataset_val,callbacks=[reduce_lr],verbose=2,epochs=1)#模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af5e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_accuracy'],label='val_acc')#这个是用来测试集的准确度提升曲线图，x轴是轮数，y轴是accuracy（准确度）\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba01f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')#保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b76533",
   "metadata": {},
   "source": [
    "模型读取+验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f5a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0cce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_file = 'model.h5'\n",
    "model_read = tf.keras.models.load_model(keras_file)#把模型导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc97016",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = glob.glob('Dataset-test/Recyclable_trash/*')#测试的文件路径\n",
    "num=0\n",
    "for n in range(30):#选前30张图片来进行测试\n",
    "    print(test_path[n])\n",
    "    image = cv2.imread(test_path[n])#读取图像，里面参数是图片的路径\n",
    "    image = cv2.resize(image, (64, 64))#将读取的图片resize成64乘64的大小\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)#因为opencv读取出来的是BGR图像，转化成RGB图像\n",
    "    image_bn = image.astype(\"float32\") / 255.0#数据归一化\n",
    "    image = np.expand_dims(image, axis=0)#拓展图片维度\n",
    "    image_bn = np.expand_dims(image_bn, axis=0)\n",
    "    pred = model.predict(image_bn)#使用模型进行预测\n",
    "    max_index = np.argmax(pred)#将模型对4类的预测中，概率最大的数值提取出来\n",
    "    print(max_index)\n",
    "    if max_index==1:\n",
    "        print('厨余垃圾')\n",
    "        num+=1\n",
    "    elif max_index==3:\n",
    "        print('可回收垃圾')       \n",
    "    elif max_index==2:\n",
    "        print('其他垃圾') \n",
    "    else:\n",
    "        print('有害垃圾')\n",
    "        num+=1\n",
    "print(num/30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28533d",
   "metadata": {},
   "source": [
    "数据集重命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path='Dataset/Recyclable_trash'    \n",
    "\n",
    "#获取该目录下所有文件，存入列表中\n",
    "fileList=os.listdir(path)\n",
    "n=0\n",
    "for i in fileList:\n",
    "    \n",
    "    #设置旧文件名（就是路径+文件名）\n",
    "    oldname=path+ os.sep + fileList[n]   # os.sep添加系统分隔符\n",
    "    \n",
    "    #设置新文件名\n",
    "    newname=path + os.sep +'rec_gla'+str(n+1)+'.jpeg'\n",
    "    \n",
    "    os.rename(oldname,newname)   #用os模块rename方法对文件改名\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536c3d8a",
   "metadata": {},
   "source": [
    "文件异常检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6afbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件可能报错，提示一堆格式错误\n",
    "# 修改数据集路径dir，和错误中出现格式如：BM\n",
    "import os\n",
    "dir = 'Dataset/Recyclable_trash/'\n",
    "for i, filename in enumerate(os.listdir(dir)):\n",
    "    filename = dir + filename\n",
    "    with open(filename, 'rb') as imageFile:\n",
    "        if imageFile.read().startswith(b'BM'):\n",
    "            print(f\"{i}: {filename} - found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
